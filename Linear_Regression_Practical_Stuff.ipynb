{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abbujaa/datascience/blob/main/Linear_Regression_Practical_Stuff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis Function\n",
        "\n",
        "In Linear Regression, the hypothesis function (also known as the model or prediction function) is a linear equation that represents the relationship between the input variables (often represented by \"x\") and the output variable (often represented by \"y\"). The equation is typically represented as:\n",
        "\n",
        "y = mx + b\n",
        "\n",
        "Where \"m\" represents the slope of the line, \"x\" represents the input variable(s), \"b\" represents the y-intercept, and \"y\" represents the output variable. The goal of linear regression is to find the best values for \"m\" and \"b\" that minimize the error between the predicted values and the actual values of the output variable.\n",
        "\n",
        "However, we can have other notational changes, so intead of b we can write b0 and instead of m we can write b1.\n",
        "\n",
        "In multiple linear regression, the hypothesis function will have multiple input variable, the equation will be represented as:\n",
        "\n",
        "y = b0 + b1*x1 + b2*x2 + … + bn*xn\n",
        "\n",
        "Where \"b0\" represents the y-intercept, \"b1\", \"b2\", ..., \"bn\" represents the coefficient of each input variable x1, x2, ... xn respectively."
      ],
      "metadata": {
        "id": "pNKtCeVnvHUU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HembG17uy_H",
        "outputId": "e2b15c02-f389-46d5-b7a3-6eaea0365086"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 9 19 29]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def predict(parameters, x):\n",
        "    x = np.insert(x, 0, 1, axis=1)\n",
        "    y_hat = np.dot(x, parameters)\n",
        "    return y_hat\n",
        "\n",
        "# Example input data\n",
        "x = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "# Example parameters\n",
        "parameters = np.array([1, 2, 3])\n",
        "\n",
        "# Get predictions\n",
        "predictions = predict(parameters, x)\n",
        "\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of above output**:-\n",
        "\n",
        "Here the input data 'x' is a 2-dimensional numpy array containing 3 samples, where each sample has 2 features.\n",
        "The parameters is a 1-dimensional numpy array containing 3 values, these values are coefficients of the hypothesis function.\n",
        "\n",
        "The output will be a 1-dimensional numpy array with 3 predictions, one for each sample in the input.\n",
        "\n",
        "You can also validate the output by calculating the dot product of input and parameters on your own.\n",
        "\n",
        "Note that in this example, the input data does not include the column of 1s for the y-intercept term, so the predict function will insert that column."
      ],
      "metadata": {
        "id": "sIAZV-bBvp0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Case-Study\n",
        "\n",
        "The problem is to predict the sales of a product based on the amount spent on TV, radio, and newspaper advertising. We have historical data that includes the advertising budget for each medium (TV, radio, and newspaper) and the corresponding sales figures. We will use Linear regression to find the relationship between the advertising budget and the corresponding sales figures. The goal is to find the best set of parameters of the linear model that can be used to predict sales for new advertising budgets. The input for the model will be the budget for the advertising in the three mediums, and the output will be the corresponding sales. By using the Linear Regression model, we will be able to predict the sales for any given advertising budget in future."
      ],
      "metadata": {
        "id": "7MLwNtgBwMKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "advertising_df = pd.read_csv(\"/content/drive/MyDrive/CS001-B03 Notebooks/data/Advertising.csv\")\n",
        "print(advertising_df.head())\n",
        "print(advertising_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "la11-hLJvaqj",
        "outputId": "d9db2d3a-0eb7-44a8-b43c-449ce8a3d93c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      TV  Radio  Newspaper  Sales\n",
            "0  230.1   37.8       69.2   22.1\n",
            "1   44.5   39.3       45.1   10.4\n",
            "2   17.2   45.9       69.3   12.0\n",
            "3  151.5   41.3       58.5   16.5\n",
            "4  180.8   10.8       58.4   17.9\n",
            "(200, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = advertising_df[['TV', 'Radio', 'Newspaper']]\n",
        "y = advertising_df['Sales']\n",
        "\n",
        "parameters = np.random.rand(x.shape[1]+1)\n",
        "predictions = predict(parameters, x.values)\n",
        "print(parameters)\n",
        "print(predictions[0:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQ_kunWGxwnG",
        "outputId": "55ff5c89-38b6-4919-ad59-b1433a7f3295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.74064913 0.53778626 0.77732263 0.69871949]\n",
            "[202.21945234  86.73316616  94.09094215 155.19378273 147.17270804]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, I'm using the pandas library to load the advertising data from a CSV file  \n",
        "\n",
        "Then I'm selecting the input variables 'TV', 'Radio', 'Newspaper' and the output variable 'Sales' from the loaded dataframe. Then, I'm converting the input variables to numpy array by calling the values method. Then rest of the code is same as the previous example, generating random parameters and passing it to the predict function.\n",
        "\n",
        "\n",
        "Please note that this is just an example and the predictions made by the model with random parameters will not be accurate because the model has not been trained with any kind of optimization techniques. To get accurate predictions, you need to train the model with appropriate optimization techniques like gradient descent."
      ],
      "metadata": {
        "id": "VZhGf3tvyVsf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cost Function\n",
        "\n",
        "In linear regression, the goal is to find the best set of parameters (coefficients) of the linear model that can be used to predict the output variable based on the input variables. The cost function is a measure of how well the model is able to make predictions for the given data.\n",
        "\n",
        "The most commonly used cost function in linear regression is the mean squared error (MSE) cost function. The MSE cost function measures the average squared difference between the predicted output and the actual output for all the samples in the data.\n",
        "\n",
        "The MSE cost function is defined as:\n",
        "\n",
        "1/2m * ∑(y^ - y)^2\n",
        "\n",
        "Where \"m\" is the number of samples, \"y^\" is the predicted output, \"y\" is the actual output and the summation is done over all the samples.\n",
        "\n",
        "The factor 1/2m is included in the cost function to make the derivative of the cost function simpler.\n",
        "\n",
        "The goal of linear regression is to find the set of parameters (coefficients) that minimize the MSE cost function. The parameters that minimize the cost function are considered to be the best fit for the data and can be used to make predictions on new data.\n",
        "\n",
        "The cost function value can be interpreted as the average of the squared difference between the predicted and actual values, therefore a lower cost function value means the model is making more accurate predictions on the data it was trained on, which is what we want to achieve."
      ],
      "metadata": {
        "id": "C1KDGDRNzkYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_function(parameters, x, y):\n",
        "    y_hat = np.dot(x, parameters)\n",
        "    cost = np.mean((y - y_hat) ** 2)\n",
        "    cost = cost/(2*len(y))\n",
        "    return cost\n",
        "\n",
        "# Get cost/error\n",
        "cost = cost_function(parameters, x.values, y.values)\n",
        "\n",
        "print(cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeWwdD_yx4Pe",
        "outputId": "cbfb2453-6348-4e08-e130-239250b536b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "52.7588402734372\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x['ones'] = np.ones(len(y))\n"
      ],
      "metadata": {
        "id": "j1iCBGTj-YvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent\n",
        "\n",
        "Gradient descent is an optimization algorithm that is commonly used to minimize a cost function in machine learning. It is used to find the set of parameters (such as weights and biases) that minimize the cost function for a given dataset.\n",
        "\n",
        "The basic idea behind gradient descent is to start with an initial set of parameters, and then iteratively update the parameters in the direction of the negative gradient of the cost function. The negative gradient points in the direction of the steepest decrease in the cost function, and so the parameters are moved in that direction.\n",
        "\n",
        "In simple terms, gradient descent works by iteratively moving in the opposite direction of the slope of the cost function. The algorithm starts at the top of the cost function, and at each step, it moves a little bit in the opposite direction of the slope, until it reaches the bottom of the cost function. The bottom is the place where the cost function is minimum and the model's prediction are best.\n",
        "\n",
        "The process of updating the parameters is repeated until the cost function converges to a minimum value or reaches a certain number of iterations. Once the best set of parameters are found, the model can be used to make predictions on new data by passing it through the hypothesis function with the found parameters."
      ],
      "metadata": {
        "id": "RlGKXR9E530Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(x, y, alpha, num_iters):\n",
        "    # initialize parameters\n",
        "    parameters = np.random.rand(x.shape[1])\n",
        "    # run gradient descent\n",
        "    for i in range(num_iters):\n",
        "        y_hat = np.dot(x, parameters)\n",
        "        error = y_hat - y\n",
        "        gradient = np.dot(x.T, error) / len(y)\n",
        "        parameters = parameters - alpha * gradient\n",
        "    return parameters\n",
        "\n",
        "parameters =  gradient_descent(x,y,0.0001, 100)"
      ],
      "metadata": {
        "id": "xAP6m9nKz_wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the gradient descent function that I provided, the line gradient = np.dot(x.T, error) / m calculates the gradient of the cost function with respect to the parameters.\n",
        "\n",
        "Here's how the line works:\n",
        "\n",
        "`x.T`: Takes the transpose of the input data, this is done to make the shapes of x and error compatible for taking dot product\n",
        "`error` : is a 1-dimensional numpy array which contains the difference between the predicted output and the actual output for all the samples\n",
        "`np.dot(x.T, error)` : takes the dot product of the transpose of the input variables and the error, which gives the summation of the product of error and input variables for each sample.\n",
        "`/m` : Dividing the summation by the number of samples (m) to get the average gradient.\n",
        "\n",
        "The gradient is a n+1 dimensional vector which contains the partial derivative of the cost function with respect to each parameter. The gradient points in the direction of the steepest increase in the cost function, and so the parameters are moved in the opposite direction (i.e., towards the direction of negative gradient) to minimize the cost function."
      ],
      "metadata": {
        "id": "qNnUzU0c6xB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = np.dot(x, parameters)"
      ],
      "metadata": {
        "id": "YnLmZZWR6xoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost_advertising = cost_function(parameters, x.values, y.values)"
      ],
      "metadata": {
        "id": "FcDtFNVL_kUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost_advertising"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oFYu8f9BbQW",
        "outputId": "e774589e-e5de-4d5a-a63d-470273594d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.750290364655798e+62"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Closed Form Solution\n",
        "\n",
        "The closed-form solution is an analytical method to find the best set of parameters for the linear regression model. It is also called the normal equation method. It involves finding the inverse of the matrix of input variables and taking the dot product of that matrix with the output variable.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yuCXITirDK2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def closed_form_solution(x, y):\n",
        "    # add a column of ones to the input variables (for the y-intercept term)\n",
        "    # calculate the inverse of the dot product of x transpose and x\n",
        "    x_inv = np.linalg.inv(np.dot(x.T, x))\n",
        "    # calculate the dot product of x transpose and y\n",
        "    x_ty = np.dot(x.T, y)\n",
        "    # calculate the optimal parameters\n",
        "    parameters = np.dot(x_inv, x_ty)\n",
        "    return parameters\n",
        "parameters = closed_form_solution(x, y)\n"
      ],
      "metadata": {
        "id": "_s5ukXVRB6sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfAzPaaKDuht",
        "outputId": "2c575da6-879e-4d2a-9daa-567e422f359c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.44457803e-02, 1.07001228e-01, 3.35657922e-04, 4.62512408e+00])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### T-Test for testing significance of individual regression coef\n",
        "\n",
        "This example uses the scipy library to perform the t-test. The standard error of the coefficient is calculated first by finding the square root of the diagonal of the inverse of the dot product of the transpose of the input variables and the input variables. The t-value is then calculated by dividing the coefficient by the standard error. Finally, the p-value is calculated using the survival function (sf) of the t-distribution, with the absolute value of the t-value and the degrees of freedom (len(y)-2)."
      ],
      "metadata": {
        "id": "2E2iu6g4EG_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "# calculate the predicted values\n",
        "y_hat = np.dot(x, parameters)\n",
        "\n",
        "# calculate the residuals\n",
        "residuals = y - y_hat\n",
        "\n",
        "# calculate the residual sum of squares\n",
        "rss = np.sum(residuals ** 2)\n",
        "\n",
        "# calculate the degrees of freedom\n",
        "df = len(y) - len(parameters)\n",
        "\n",
        "# calculate the mean squared error\n",
        "mse = rss / df\n",
        "\n",
        "# calculate the standard error of the coefficients\n",
        "se = np.sqrt(np.diag(mse * np.linalg.inv(np.dot(x.T, x))))\n",
        "\n",
        "# calculate the t-values\n",
        "t_values = parameters / se\n",
        "\n",
        "# calculate the p-values\n",
        "p_values = (1 - stats.t.cdf(np.abs(t_values), df)) * 2\n"
      ],
      "metadata": {
        "id": "KvYMwrWyDz78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_values"
      ],
      "metadata": {
        "id": "C0yYJ5WmIhb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2470146c-1da8-4635-fff2-b9d08157592e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.       , 0.       , 0.9538145, 0.       ])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_significant_coefficients(p_values, significance_level=0.05):\n",
        "    significant_coefficients = []\n",
        "    for i, p_value in enumerate(p_values):\n",
        "        if p_value < significance_level:\n",
        "            significant_coefficients.append(i)\n",
        "    return significant_coefficients\n",
        "\n",
        "# Example usage:\n",
        "significant_coefficients = identify_significant_coefficients(p_values)\n",
        "print(significant_coefficients)\n",
        "# Output: [0, 1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylHsft914i5f",
        "outputId": "cc975b4c-1d56-4fb7-91c1-d122331313d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yOlcPh886AL2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}